# Import packages
import Packages.dm_control
from Packages.dm_control.dm_control import suite
from Packages.dm_control.dm_control import viewer
from Packages.dm_control.dm_control.suite.wrappers import pixels
from Packages.dm_env.dm_env import specs
from PIL import Image, ImageTk
import subprocess
import numpy as np
import matplotlib.pyplot as plt
import itertools
import inspect
from SAC.sac_torch import Agent
from SAC.utils import plot_learning_curve
import os
import torch as T
import pandas as pd
import sys
np.set_printoptions(threshold=sys.maxsize)
import gc
import gym

from stable_baselines3 import SAC


if __name__ == '__main__':
   
    # Define environment
    env_id = 'cloth_corner'
    n_games = 50
    score_max = []
    time_step_counter_max = []

    # Load environment
    env = suite.load(domain_name=env_id, task_name="easy")
    env = gym.make('dm2gym:CheetahRun-v0')
    # Define action space and action shape
    # action_space = env.action_spec()
    # action_shape = action_space.shape[0]

    # env.action_space = action_space
    # Define observation space and observation shape
    # observation_space = env.observation_spec()
    # observation_shape = observation_space['position'].shape
    print(action_space)
    print(observation_shape)

    env.observation_space = observation_space
    env.reward_range = (0,1)
    metadata = {'render.modes': []}
    env.metadata = metadata

    # create output folder for current experiment
    path_to_output = os.getcwd() + '/output'
    os.chdir(path_to_output)
    
    if not os.path.exists(env_id):
        subprocess.call(["mkdir","-p", env_id])

    # create experiment
    path_to_environment = path_to_output + '/' + env_id
    os.chdir(path_to_environment)

    if not os.path.exists('checkpoint_tmp'):
        subprocess.call(["mkdir","-p", 'checkpoint_tmp'])

    # Path to all intermediate checkpoints
    path_to_checkpoint = path_to_environment + '/checkpoint_tmp'

    # Load models from checkpoint
    load_checkpoint = False
    if load_checkpoint:
        agent.load_models()
        #viewer.launch(env)
    
    # Define log dictionary  
    log_dict = {
                "Image" : [],
                "Game_no" : [],
                "Step_no" : [],
                "State" : [],
                "Action" : [],
                "Reward" : [],
                }
    log_sac_dict = {
                "Reward" : [],
                "Value" : [],
                "Critic" : [],
                "Target_value" : [],
                "Entropy" : []
                }
    log_sac_loss_dict = {
                "Value_loss" : [],
                "Critic_loss" : [],
                "Actor_loss" : [],
                "Target_value_loss" : []
                }
    """ Consumes too much memory. So not saving inputs anymore.
    log_sac_dict_inputs = {
                "State" : [],
                "Next_State" : [],
                "Action" : [],
                "Sampled_Action" : []
                }
    """
    # Loop for a no. of games
    for i in range(n_games):
        
        # Reset environment
        time_step = env.reset()
        observation = time_step.observation['position']
        observation['shape'] = observation_shape
        reward_history = []
        average_value = []
        average_critic = []
        average_entropy = []
        average_reward = []
        average_target_value = []
        step = 0
        reward = 0
        done = False
        print("GAME no.",i,"/n")
        for init_step in range(500):
            # env.step(np.array([0.,0.])) # for cloth_sewts_exp2_2
            env.step(np.array([0.,0.,0.])) # for cloth_corner
            # env.step(np.array([0.]*7)) # for cloth_corner random_location = False
        
        # Change directory to current environment path
        os.chdir(path_to_environment)
        
        # Make folder for each game
        game_no = 'game_' + str(i+1)
        subprocess.call([ 'mkdir', '-p', game_no ])

        # Display current game no.
        # print("GAME NO.", i,"\n")

        log_file = path_to_environment + "/log"+str(i)+".csv"
        log_sac_file = path_to_environment + "/log_sac"+str(i)+".csv"
        log_sac_file_inputs = path_to_environment + "/log_sac_inputs"+str(i)+".csv"
        log_sac_loss_file = path_to_environment + "/log_sac_loss"+str(i)+".csv"
        model = SAC("MlpPolicy", env, verbose=1)
        print("MODEL")
        print(model)
        while done is False : 

            # Move to game folder
            # gc.collect()
            path_to_game = path_to_environment + '/' + game_no
            os.chdir(path_to_game)
            
            # Take action 
                    
            # action = np.array([0.1,0.1])
            # observation = observation / observation.max()
            action, observation_ = model.predict(observation, deterministic=True)
            print("STEP",i,"/n")
        
            print(action,"/n")
            # print(observation,"/n")
            action_fed = np.array([action[0][0], action[0][1], action[0][2]]) #cloth_corner
            # action_fed = np.array([action[0], action[1]]) #cloth_sewts_exp2_2
            # action_fed = np.array([action[i]] for i in range(len(action))) #cloth_corner corner + action
            # tion_fed = action
            print("Action fed /n")
            print(action_fed)
            # action[2] = 0. # Uncomment for cloth_sewts_v1 / v2 env
            time_step = env.step(action_fed)
            obs, reward, done, info = env.step(action)

            # INFERENCE IN SIMULATION !
            # if load_checkpoint:
            #    simulation(action_fed)
            
            
            # print("TIME STEP\n")
            # print(time_step)

            # Get next observation and reward
            observation_ = time_step.observation['position']
            reward = time_step.reward
            print(observation_,"/n")
            print(reward,"/n")
            # print(observation) # Print oberservation
            # print(reward) # Print reward

            # Render image from environment for the time step and save
            viewer.launch(env)


            model.learn(total_timesteps=10000)



            image_data = env.physics.render(width = 64, height = 64 , camera_id = 0)
            #img = Image.open(image_data)
            # image_array = np.asarray(image_data)
            # print("Saving frames")
            img = Image.fromarray(image_data, 'RGB')
            img_loc = path_to_game + "/frame-%.10d.png" % step
            img_name = "frame-%.10d.png" % step
            img.save(img_name)
            
            # print(img_loc)
            # print(os.getcwd())
            # img.show(img)
            

            # Increment step
            step += 1

            # Define terminal state (Max no. of steps 100 or when we reach close to the maximum reward of 0)
            if step == 1000 or reward > 9.0 and step > 250: # cloth_corner
            # if step == 10000 or reward > 9.0 and step > 1000: # cloth_sewts_exp2_2
            # if step == 10000 or reward > -0.005: # No. of steps should be > batch size of 250 as the agent learns only when the batch is full
                done = True

            # Add current observation, action, reward and next observation to the Replay Buffer
            agent.remember(observation, action, reward, observation_, done)
            # gc.collect() 
            # Learn parameters of SAC Agent
            # if not load_checkpoint:    
                # agent.value.fc1.weight.data.fill_(0.1)
                # agent.value.fc1.bias.data.fill_(0.1)
                # agent.value.fc2.weight.data.fill_(0.1)
                # agent.value.fc2.bias.data.fill_(0.1)
                # agent.value.v.weight.data.fill_(0.1)
                # agent.value.v.bias.data.fill_(0.1)
                # T.nn.init.uniform_(T.empty(3, 5))
                #obs_trial = [0]*12
                #obs_trial = T.tensor(obs_trial)
                #print("OBS TRIAL \n")
                #print(agent.value(obs_trial))

            T.save(agent.value.state_dict(), os.path.join(path_to_environment,'checkpoint_tmp/init_value.ckpt'))
            T.save(agent.actor.state_dict(), os.path.join(path_to_environment, 'checkpoint_tmp/init_actor.ckpt'))
            T.save(agent.critic_1.state_dict(),  os.path.join(path_to_environment,'checkpoint_tmp/init_critic.ckpt'))
            T.save(agent.target_value.state_dict(), os.path.join(path_to_environment,'checkpoint_tmp/init_target_value.ckpt'))
            agent.learn()
             
            # Update observation with next observation
            observation = observation_
            # gc.collect()
            # Add to the list of rewards for each time step 
            reward_history.append(reward)
            
            # Add a logger to capture states, rewards, actions coupled with the images captured to understand how the agent operates

            log_dict["Image"].append(img_loc)
            log_dict["Game_no"].append(i + 1)
            log_dict["Step_no"].append(step)
            log_dict["State"].append(observation)
            log_dict["Action"].append(action)
            log_dict["Reward"].append(reward)
            
            if agent.learn() is not None:
                # print("PRINTING :: : :: : :: : :: : :")
                # print(agent.learn()[0].detach().cpu().numpy())
                # log_sac_dict_inputs["State"].append(agent.learn()[0].detach().cpu().numpy())
                # log_sac_dict_inputs["Next_State"].append(agent.learn()[1].detach().cpu().numpy())
                # log_sac_dict_inputs["Action"].append(agent.learn()[2].detach().cpu().numpy())
                # log_sac_dict_inputs["Sampled_Action"].append(agent.learn()[3].detach().cpu().numpy())
                log_sac_dict["Reward"].append(agent.learn()[4].detach().cpu().numpy())
                # RE-ADD
                log_sac_dict["Value"].append(agent.learn()[5].detach().cpu().numpy())
                log_sac_dict["Target_value"].append(agent.learn()[6].detach().cpu().numpy())
                log_sac_dict["Critic"].append(agent.learn()[7].detach().cpu().numpy())
                log_sac_dict["Entropy"].append(agent.learn()[8].detach().cpu().numpy())
                log_sac_loss_dict["Value_loss"].append(agent.learn()[9].detach().cpu().numpy())
                log_sac_loss_dict["Critic_loss"].append(agent.learn()[10].detach().cpu().numpy())
                log_sac_loss_dict["Actor_loss"].append(agent.learn()[11].detach().cpu().numpy())
                # RE-ADD END
            else:
                # log_sac_dict_inputs["State"].append(None)
                # log_sac_dict_inputs["Next_State"].append(None)
                # log_sac_dict_inputs["Action"].append(None)
                # log_sac_dict_inputs["Sampled_Action"].append(None)
                log_sac_dict["Reward"].append(None)
                # RE-ADD
                log_sac_dict["Value"].append(None)
                log_sac_dict["Target_value"].append(None)
                log_sac_dict["Critic"].append(None)
                log_sac_dict["Entropy"].append(None)
                log_sac_loss_dict["Value_loss"].append(None)
                log_sac_loss_dict["Critic_loss"].append(None)
                log_sac_loss_dict["Actor_loss"].append(None)
                # RE-ADD END
            # gc.collect() 
            
        # Save agent models
            if not load_checkpoint:
                agent.save_models()

            """ No need to save intermediate checkpoints now. Save memory !
            intermediate_ckpt_value = path_to_checkpoint + '/ckpt_value_' + str(step) + '.ckpt'
            if not load_checkpoint:
                T.save(agent.value.state_dict(), intermediate_ckpt_value)

            intermediate_ckpt_critic_1 = path_to_checkpoint + '/ckpt_critic_1' + str(step) + '.ckpt'
            if not load_checkpoint:
                T.save(agent.critic_1.state_dict(), intermediate_ckpt_critic_1)

            intermediate_ckpt_critic_2 = path_to_checkpoint + '/ckpt_critic_2' + str(step) + '.ckpt'
            if not load_checkpoint:
                T.save(agent.critic_2.state_dict(), intermediate_ckpt_critic_2)
 
            intermediate_ckpt_actor = path_to_checkpoint + '/ckpt_actor_' + str(step) + '.ckpt'
            if not load_checkpoint:
                T.save(agent.actor.state_dict(), intermediate_ckpt_actor)       # Get maximum reward for the current game
            """
        score_max.append(np.amax(reward_history))
        #print("MAX REWARD FOR GAME NO.", i, "/n")
        #print(score_max)

        # Define filename to store plots
        filename = env_id + '_'+ str(n_games) + 'plot.png'
        figure_file = filename
        figure_file = os.path.join(os.getcwd(), figure_file)

        # Plot learning curves

        subprocess.call([ 'mkdir', '-p', 'Plots' ])
        os.chdir('Plots')
        
        # gc.collect()
        
        # No need to plot entropy, reward, value, target value for each batch running. Was needed when solving NaN problem. But now it adds unnecessary computation and contributes to OOM error.
        x = [i for i in range(step)]
        plot_learning_curve(x, reward_history, figure_file)
        # RE-ADD
        for i in range(len(log_sac_dict["Entropy"])):
            filename_entropy = env_id + '_'+ str(i) + 'plot_entropy.png'
            figure_file_entropy = filename_entropy
            figure_file_entropy = os.path.join(os.getcwd(), figure_file_entropy)
            if(log_sac_dict["Entropy"][i]) is not None:
                step_size = []
                length = len(log_sac_dict["Entropy"][i])
                average_entropy.append(sum(log_sac_dict["Entropy"][i]) / len(log_sac_dict["Entropy"][i]))
                # for j in range(length):
                #    step_size.append(j)
                # plot_learning_curve(step_size, log_sac_dict["Entropy"][i], figure_file_entropy)

        for i in range(len(log_sac_dict["Value"])):
            filename_value = env_id + '_'+ str(i) + 'plot_value.png'
            figure_file_value = filename_value
            figure_file_value = os.path.join(os.getcwd(), figure_file_value)
            if(log_sac_dict["Value"][i]) is not None:
                step_size = []
                length = len(log_sac_dict["Value"][i])
                average_value.append(sum(log_sac_dict["Value"][i]) / len(log_sac_dict["Value"][i]))
                # for j in range(length):
                #     step_size.append(j)
                # plot_learning_curve(step_size, log_sac_dict["Value"][i], figure_file_value)      
                
        for i in range(len(log_sac_dict["Target_value"])):
            filename_target_value = env_id + '_'+ str(i) + 'plot_target_value.png'
            figure_file_target_value = filename_target_value
            figure_file_target_value = os.path.join(os.getcwd(), figure_file_target_value)
            if(log_sac_dict["Target_value"][i]) is not None:
                step_size = []
                length = len(log_sac_dict["Target_value"][i])
                average_target_value.append(sum(log_sac_dict["Target_value"][i]) / len(log_sac_dict["Target_value"][i]))
                # for j in range(length):
                #    step_size.append(j)
                # plot_learning_curve(step_size, log_sac_dict["Target_value"][i], figure_file_target_value)      
        
        for i in range(len(log_sac_dict["Critic"])):
            filename_critic = env_id + '_'+ str(i) + 'plot_critic.png'
            figure_file_critic = filename_critic
            figure_file_critic = os.path.join(os.getcwd(), figure_file_critic)
            if(log_sac_dict["Critic"][i]) is not None:
                step_size = []
                length = len(log_sac_dict["Critic"][i])
                average_critic.append(sum(log_sac_dict["Critic"][i]) / len(log_sac_dict["Critic"][i]))
                # for j in range(length):
                #     step_size.append(j)
                # plot_learning_curve(step_size, log_sac_dict["Critic"][i], figure_file_critic)  

        for i in range(len(log_sac_dict["Reward"])):
            filename_reward = env_id + '_'+ str(i) + 'plot_reward.png'
            figure_file_reward = filename_reward
            figure_file_reward = os.path.join(os.getcwd(), figure_file_reward)
            if(log_sac_dict["Reward"][i]) is not None:
                step_size = []
                log_sac_dict["Reward"][i] = np.array(log_sac_dict["Reward"][i])               
                length = len(log_sac_dict["Reward"][i])
                # print("REWARD")
                # print(log_sac_dict["Reward"])
                average_reward.append(sum(log_sac_dict["Reward"][i]) / len(log_sac_dict["Reward"][i]))
                # for j in range(length):
                #     step_size.append(j)
                # print(log_sac_dict["Reward"][i].dtype) 
                # plot_learning_curve(step_size, log_sac_dict["Reward"][i], figure_file_reward)  
       
        # gc.collect()

        step_size = []
        #if(log_sac_loss_dict["Value_loss"]) is not None:
        length = len(log_sac_loss_dict["Value_loss"])
        # print(log_sac_loss_dict["Value_loss"])
        for j in range(length):
            step_size.append(j)
            if(log_sac_loss_dict["Value_loss"][j]):
                if(isinstance(log_sac_loss_dict["Value_loss"][j], np.ndarray)):
                    log_sac_loss_dict["Value_loss"][j] = log_sac_loss_dict["Value_loss"][j].item()
            else:
                log_sac_loss_dict["Value_loss"][j] = 0 
            if(log_sac_loss_dict["Critic_loss"][j]):
                if(isinstance(log_sac_loss_dict["Critic_loss"][j], np.ndarray)):
                    log_sac_loss_dict["Critic_loss"][j] = log_sac_loss_dict["Critic_loss"][j].item()
            else:
                 log_sac_loss_dict["Critic_loss"][j] = 0
            if(log_sac_loss_dict["Actor_loss"][j]):
                if(isinstance(log_sac_loss_dict["Actor_loss"][j], np.ndarray)):
                    log_sac_loss_dict["Actor_loss"][j] = log_sac_loss_dict["Actor_loss"][j].item()
            else:
                log_sac_loss_dict["Actor_loss"][j] = 0         
                       
        print("STEP !!!!!!!!!!!!!!")
        # print(step_size)
        # print(log_sac_loss_dict["Value_loss"])
        filename_value_loss = env_id + '_'+ str(i) + 'plot_value_loss.png'
        figure_file_value_loss = filename_value_loss
        figure_file_value_loss = os.path.join(os.getcwd(), figure_file_value_loss)
        plot_learning_curve(step_size, log_sac_loss_dict["Value_loss"], figure_file_value_loss)  
        
        
        filename_critic_loss = env_id + '_'+ str(i) + 'plot_critic_loss.png'
        figure_file_critic_loss = filename_critic_loss
        figure_file_critic_loss = os.path.join(os.getcwd(), figure_file_critic_loss)
        plot_learning_curve(step_size, log_sac_loss_dict["Critic_loss"], figure_file_critic_loss)  
        
        filename_actor_loss = env_id + '_'+ str(i) + 'plot_actor_loss.png'
        figure_file_actor_loss = filename_actor_loss
        figure_file_actor_loss = os.path.join(os.getcwd(), figure_file_actor_loss)
        plot_learning_curve(step_size, log_sac_loss_dict["Actor_loss"], figure_file_actor_loss)  
        
        filename = str(n_games)
        subprocess.call([
                        'ffmpeg', '-framerate', '50', '-y', '-i',
                        'frame-%010d.png', '-r', '30', '-pix_fmt', 'yuv420p','video.mp4'                
                        ])

        # os.chdir(path_to_environment)
        
        filename_average_critic = str(i) + 'plot_average_critic.png'
        figure_file_average_critic = filename_average_critic
        figure_file_average_critic = os.path.join(os.getcwd(), figure_file_average_critic)

        filename_average_value = str(i) + 'plot_average_value.png'
        figure_file_average_value = filename_average_value
        figure_file_average_value = os.path.join(os.getcwd(), figure_file_average_value)       
 
        filename_average_target_value = str(i) + 'plot_average_target_value.png'
        figure_file_average_target_value = filename_average_target_value
        figure_file_average_target_value = os.path.join(os.getcwd(), figure_file_average_target_value)       

        filename_average_entropy = str(i) + 'plot_average_entropy.png'
        figure_file_average_entropy = filename_average_entropy
        figure_file_average_entropy = os.path.join(os.getcwd(), figure_file_average_entropy)
 
        filename_average_reward = str(i) + 'plot_average_reward.png'
        figure_file_average_reward = filename_average_reward
        figure_file_average_reward = os.path.join(os.getcwd(), figure_file_average_reward)
        
        steps_avg = []
        for x in range(len(average_value)):
            steps_avg.append(x)
        
        # print(steps_avg)
        # print(average_entropy)
        # print(figure_file_average_entropy)

        plot_learning_curve(steps_avg, average_entropy, figure_file_average_entropy)  
        plot_learning_curve(steps_avg, average_critic, figure_file_average_critic)  
        plot_learning_curve(steps_avg, average_value, figure_file_average_value)  
        plot_learning_curve(steps_avg, average_target_value, figure_file_average_target_value) 
        plot_learning_curve(steps_avg, average_reward, figure_file_average_reward)  
   
        # Uncomment when you want to save all important values in a CSV
        # In case you want to save the log_dict in a separate log.txt file
        print("Saving CSVs ... /n")
        df = pd.DataFrame(log_dict).to_csv(log_file, header = True, index = False)
        # print(log_sac_dict.keys())
        df = pd.DataFrame(log_sac_dict).to_csv(log_sac_file, header = True, index = False)
        # df = pd.DataFrame(log_sac_dict_inputs).to_csv(log_sac_file_inputs, header = True, index = False)
        # df = pd.DataFrame(log_sac_loss_dict).to_csv(log_sac_loss_file, header = True, index = False)
            
        log_dict = {key: [] for key in log_dict}
        log_sac_loss_dict = {key: [] for key in log_sac_loss_dict}
        log_sac_dict = {key: [] for key in log_sac_dict}
        # log_sac_dict_inputs = {key: [] for key in log_sac_dict_inputs}
        print("Saved CSVs ... /n")     
        #log_sac_loss_dict.clear()
        #log_sac_dict.clear()
        #log_sac_dict_inputs.clear()
#"""
    # step_x = [i+1 for i in range(n_games)]
    # plot_learning_curve(step_x, score_max, figure_file)



    # print("Reward")
    # print(len(log_sac_dict["Reward"][260]))
    # print("Value \n")
    # print(len(log_sac_dict["Value"][260]))
    # print("Target Value \n")
    # print(len(log_sac_dict["Target_Value"][260]))
    # print("Critic \n")
    # print(len(log_sac_dict["Critic"][260]))
    # print("Entropy  \n")   
    # print(len(log_sac_dict["Entropy"][260]))
    # df.to_csv(log_file)
    # df = pd.read_csv(log_file, index_col=0)
    # print(df)
    # file_csv = open(log_file, "w")
    # text_file = csv.writer(file_csv)
    # log_dict = zip(log_dict.items())
    # for key in log_dict.items():
    #     text_file.writerow([key])
    #for value in log_dict.items():
    #    text_file.writerow([value])
    # file_csv.close()

    # Define filenames to store network output plots
    # os.chdir(path_to_environment)
    # path_to_plots = path_to_environment + '/Plots/'
    # print(path_to_plots)

    # if not os.path.exists(path_to_plots):
    #    subprocess.call(["mkdir","-p", "Plots"])

       
    # filename_critic = env_id + '_'+ str(n_games) + 'plot_critic.png'
    # figure_file_critic = filename_critic
    # figure_file_critic = os.path.join(path_to_plots, figure_file_critic)

    # filename_target_value = env_id + '_'+ str(n_games) + 'plot_target_value.png'
    # figure_file_target_value = filename_target_value
    # figure_file_target_value = os.path.join(path_to_plots, figure_file_target_value)

    # filename_entropy = env_id + '_'+ str(n_games) + 'plot_entropy.png'
    # figure_file_entropy = filename_entropy
    # figure_file_entropy = os.path.join(path_to_plots, figure_file_entropy)

    # Plot outputs of all neural networks implemented in SAC
    # value = agent.value_collect
    # critic = agent.critic_collect
    # target_value = agent.target_value_collect
    # entropy = agent.entropy_collect
    # learn_step = agent.learn_step

    # print(value)
    # print(critic)
    # print(target_value)
    # print(entropy)
    # print(learn_step)

 
    #print("PLOTS\n")
    #for j in range(len(value)):
    #    value_plot = []
    #    value_step = []     
    #    filename_value = env_id + '_'+ str(j) + 'plot_value.png'
    #    figure_file_value = filename_value
    #    figure_file_value = os.path.join(path_to_plots, figure_file_value)

    #    for i in range(len(value[j])):
    #        value_plot.append(value[j].detach().numpy())
    #        value_step.append(i)
    #    plot_learning_curve(value_plot, value_step , figure_file_value)

    #print(value)
    #print(critic)
    #print(target_value)
    #print(entropy)
    #print(learn_step)


    #plot_learning_curve(learn_step, critic, figure_file_critic)
    #plot_learning_curve(learn_step, target_value, figure_file_target_value)
    #plot_learning_curve(learn_step, entropy, figure_file_entropy)


